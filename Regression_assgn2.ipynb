{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cdf214ea-b45b-4007-9e9a-2fca30b47b33",
   "metadata": {},
   "source": [
    "1)R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that’s explained by an independent variable in a regression model.\n",
    "Formula for calculating the R2 =1-(SSres/SStotal)\n",
    "\n",
    "SSres is the distance from the best fit line to the square\n",
    "SStotal is the distance of theobserved values and the mean of the depedent variables.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "760f0f32-ddcf-4805-8200-60d89705e046",
   "metadata": {},
   "source": [
    "2)Adjusted R-squared is a modified version of the traditional R-squared in linear regression models. While R-squared measures\n",
    "the proportion of the variance in the dependent variable explained by the independent variables, adjusted R-squared adjusts \n",
    "this value to account for the number of predictors in the model. The adjusted R-squared provides a more accurate assessment \n",
    "of a model's goodness of fit, especially when comparing models with different numbers of predictors.\n",
    "\n",
    "3)Regular R-squared will never decrease when adding more predictors, even if they are irrelevant. In contrast, adjusted R-squared may decrease if the addition of a new predictor does not sufficiently improve the model's fit.\n",
    "Adjusted R-squared is often preferred when comparing models with different numbers of predictors, as it provides a more reliable measure of a model's explanatory power"
   ]
  },
  {
   "cell_type": "raw",
   "id": "846f4f2c-ae45-4db1-9950-3a7d2e6d6cd5",
   "metadata": {},
   "source": [
    "4)These are different types of error  which helps to determine how well the model has estimated the relationship between  the input features and the output."
   ]
  },
  {
   "cell_type": "raw",
   "id": "524c782c-35b7-461c-be48-f327b4adfd06",
   "metadata": {},
   "source": [
    "5)Adv of using Mse eqn is differentiable,has only one local minima \n",
    "disadv not robust to outliers and the error units is squared\n",
    "Adv of MAE robust to outliers ,it is in the same unit\n",
    "disadv convergence usually takes more time\n",
    "Adv of RMSE same unit as catagories ,and the eqn is differentiable disadv not robust to outliers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7b8a35c-41a4-4b48-ab2e-5ac3ba9a066d",
   "metadata": {},
   "source": [
    "6)Lasoo regularization also known as L1 regularization is used for feature selection cost function of lasso is :-MSE+Lamda * |slope| where in ridge regression it is used to handle overfitting cases"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e8c0f98-d316-4a34-a0d9-759d6b63a36d",
   "metadata": {},
   "source": [
    "7)Regularized linear model helps preventing overfitting in ml by adding a additional termto the standard linear regression cost function\n",
    "L1 REGULARIZATION =MSE+LAMDA|slope|\n",
    "L2regularization = MSE+lamda|slope|2\n",
    "Ridge regularization, similar to Lasso, helps prevent overfitting by penalizing large coefficients. It tends to shrink all coefficients towards zero, but it does not usually lead to exactly zero coefficients.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "377068e0-84cc-445d-9cce-75fbc979cbde",
   "metadata": {},
   "source": [
    "8)While regularized linear models, such as Lasso (L1 regularization) and Ridge (L2 regularization), are powerful tools for preventing overfitting and improving model generalization, they are not always the best choice for every regression analysis. Here are some limitations and considerations:\n",
    "\n",
    "Loss of Interpretability:\n",
    "\n",
    "Regularization methods can lead to sparse models, where some coefficients are exactly zero (especially in Lasso). While this can be advantageous for feature selection, it may result in a loss of interpretability, as certain features are completely excluded from the model.\n",
    "Selection of the Regularization Parameter:\n",
    "\n",
    "The performance of regularized models is sensitive to the choice of the regularization parameter (\n",
    "�\n",
    "λ). Selecting an appropriate value for \n",
    "�\n",
    "λ is a non-trivial task and may require cross-validation or other tuning methods. The optimal value depends on the specific characteristics of the data, and an incorrect choice can lead to underfitting or overfitting.\n",
    "Not Suitable for All Types of Data:\n",
    "\n",
    "Regularization is particularly useful when dealing with high-dimensional datasets where the number of features is large compared to the number of observations. In situations where the number of features is small or the true underlying relationship is not sparse, regularization may not provide significant benefits.\n",
    "Impact of Outliers:\n",
    "\n",
    "Regularized linear models may be sensitive to outliers. Outliers can disproportionately influence the regularization penalty and affect the resulting model. While robust variants of regularization exist, addressing outliers may require additional preprocessing steps.\n",
    "Assumption of Linearity:\n",
    "\n",
    "Regularized linear models assume a linear relationship between the features and the target variable. If the true relationship is highly nonlinear, other non-linear regression techniques or machine learning models may be more appropriate.\n",
    "Collinearity Issues:\n",
    "\n",
    "Ridge regression is effective in handling multicollinearity (high correlation among predictor variables), but it does not perform variable selection. If the dataset has highly correlated features, Lasso may perform variable selection but is sensitive to the choice of \n",
    "�\n",
    "λ.\n",
    "Computational Complexity:\n",
    "\n",
    "Solving the optimization problem with regularization involves additional computational complexity, especially when the number of features is large. While efficient algorithms exist, the computational cost may be a consideration in certain applications.\n",
    "Limited Improvements in Simple Models:\n",
    "\n",
    "Regularization is most beneficial when the model is complex or when there are many features. In simpler models with a small number of features, the benefits of regularization may be limited, and a standard linear regression model may perform well.\n",
    "Domain-Specific Considerations:\n",
    "\n",
    "The choice of whether to use regularization depends on the specific goals of the analysis and the characteristics of the data. In some domains, interpretability and understanding of the model may be more critical than predictive accuracy, and a simpler model without regularization may be preferred.\n",
    "In summary, while regularized linear models are valuable tools for many regression problems, their applicability and performance depend on the characteristics of the data and the specific goals of the analysis. It is essential to carefully consider the limitations and assess whether the assumptions and benefits of regularization align with the requirements of the regression analysis at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f04f9970-e76b-4abd-b26f-8b5dc91c0f31",
   "metadata": {},
   "source": [
    "Comparing RMSE and MAE:\n",
    "\n",
    "RMSE (Root Mean Squared Error): RMSE penalizes larger errors more than smaller ones because it involves squaring the differences between predicted and actual values. It is sensitive to outliers and tends to give more weight to them.\n",
    "\n",
    "MAE (Mean Absolute Error): MAE, on the other hand, treats all errors equally, as it involves taking the absolute values of the differences between predicted and actual values. It is less sensitive to outliers.\n",
    "\n",
    "Choice of Better Model:\n",
    "\n",
    "If our priority is to minimize the impact of larger errors and you are concerned about the effect of outliers, you might lean toward Model B with the lower MAE (8), as MAE is less influenced by extreme values.\n",
    "\n",
    "If you want a metric that gives more weight to larger errors and outliers are not a significant concern, you might prefer Model A with the lower RMSE (10)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa1ae273-78b5-4e0f-8630-e0d54b687c20",
   "metadata": {},
   "source": [
    "10)Ridge regularization adds a penalty term based on the sum of the squared values of the regression coefficients to the standard linear regression objective function.\n",
    "Ridge tends to shrink all coefficients towards zero, but it does not usually result in exactly zero coefficients.\n",
    "Lasso regularization adds a penalty term based on the sum of the absolute values of the regression coefficients to the standard linear regression objective function.\n",
    "Lasso encourages sparsity in the model and often leads to some coefficients being exactly zero, effectively performing feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
